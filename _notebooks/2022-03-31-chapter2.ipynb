{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 『pytorch ＆ 深層学習プログラミング』　備忘録chap2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "- [何故スカラーなのか（p83）](#何故スカラーなのか（p83）)\n",
    "- [何故スカラーにするために sum を用いるのか（p83）](#何故スカラーにするためにsumを用いるのか（p83）)\n",
    "\n",
    "協力：K.I.先生\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 何故スカラーなのか（p83）\n",
    "---\n",
    "<br>\n",
    "pytorchでは、勾配計算をする際に対象の関数がスカラーである必要があるらしい。正直、「スカラーにしてしまったら元の関数がわからなくなるじゃないか！」と思ってしまったのだが、どうやらpytorchはそのスカラーがどのようにして生成されたのかを記憶しているようである。\n",
    "\n",
    "pytorchの仕様として基本的にスカラーにしないと勾配計算ができないので、ここは黙ってスカラーにしておくのが無難である。\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 何故スカラーにするために sum を用いるのか（p83）\n",
    "---\n",
    "<br>\n",
    "スカラーにしてもその生前の記憶は残っていると分かったものの、次に生じた疑問は「スカラーにするだけならsum()以外の関数でも良いのでは？」というものである。\n",
    "\n",
    "試しにmean()でスカラーにしてからbackward()をかけると、結果は微分の値にはならなかった。つまり、sum()を用いてtensorの合計を求めるという計算には何か重要な意味があるのである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### ▪ 理解のための前提知識\n",
    "<br>\n",
    "\n",
    "それを理解するためには、以下の事を知っておかなければならない。\n",
    "<br>\n",
    "\n",
    "- xという「ベクトル」に対しても、x.backward() をする時に、（）に引数を入れると計算ができるということ\n",
    "- その引数は、xと同じサイズのベクターにするということ\n",
    "  \n",
    "<br>\n",
    "である。具体的に本書の例で言うと次のような意味となる。\n",
    "\n",
    "z = y.sum() してから z.backward() する代わりに、y.sum() をしないでいきなり y.backward(torch.ones(y.size())) としてもよく、実際同じ結果になる。ここで torch.ones(y.size()) は　y と同じ大きさで、全ての要素が１であるベクトルである。この計算は「全ての y を同等の weight で扱って x.grad を計算する」という意味になっている。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "###  ▪ z.backward()に隠された引数１\n",
    "<br>\n",
    "もうお気づきかもしれないが、実は z.backward() のカッコの中には 1 が隠されている。これは z を大きさ 1 のスカラーにしているので考えてみれば当然であるが、それに気づくとsum()の意味が分かってくる。y.sum()を行うことで、均等のweightで全ての要素を足し合わせたことになるのである！\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "###  ▪ 均等のweightによる足し算\n",
    "<br>\n",
    "最初はこれに突っかかってしまったのだが、それぞれの要素に何も掛け算せず（×0.8 , ×1.3 等）足し合わせているのだから、重みは当然1である。全ての重みを１にすると、微分することと同義となるのである。しかし考えてみると、機械学習やディープラーニングにおいて全ての重みを1にするなど意味のない行為である。今後、この重みを調節することによって高度な学習をするようになるのだろうか、と期待しつつ今後勉強していきたい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
