{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 『pytorch ＆ 深層学習プログラミング』　備忘録3章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "- [0以下は0にするReLU関数（p144）](#0以下は0にするReLU関数（p144）)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 0以下は0にするReLU関数（p144）\n",
    "---\n",
    "<br>\n",
    "活性化関数についての説明があり、一次関数しか生まれない線形関数の壁をReLU関数を挟むだけで打ち破ることができると、グラフを交えて説明してある。\n",
    "\n",
    "なんて画期的なんだろうと思い、その先の章のReLU関数の説明を見てみた。すると、ReLU関数とは**0以下は0に、0以上の数はそのまま返す**という単純なものであった。\n",
    "\n",
    "それだけで次元の壁を超えられるんだなあと感心しつつ、ふとひっかっかる。「今回の入力値は負の値を含む平均0、標準偏差１の乱数なのに、負の値を0なんかにして正しく予想できるのだろうか」と。\n",
    "<br>\n",
    "\n",
    "### ▪出力層を見よ\n",
    "<br>\n",
    "この疑問は、今回使用している**モデルの構造**について考えが浅いために生じていることが判明した。なんと、1つの入力から10の中間層を作っていたのである。\n",
    "\n",
    "![](image/2022-08-08-18-42-54.png)\n",
    "\n",
    "正直、１つの入力値から10の中間層を作るというイメージは湧かない。湧かないが、1つのものを細かく分解して見ているのかなと、1人の人間にも色々な臓器が入っているよねと、その程度の理解をしておく。まあ、人間ならまだしもただの数値１つとなるとやはりイメージが湧かないが仕方ない。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ef5873cd0362be76fd62dae5ce6331addfe1aa2cd79daf4ba2e2f1313b46824"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
