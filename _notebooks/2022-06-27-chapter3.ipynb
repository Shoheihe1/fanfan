{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 『pytorch ＆ 深層学習プログラミング』　備忘録3章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "- [displaの謎](#displaの謎)\n",
    "- [勾配降下法の実験（p109~）](#勾配降下法の実験（p109~） )\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## displaの謎\n",
    "---\n",
    "<br>\n",
    "テキストのコードが載っているgitからコピペをしてコードを動かそうとすると、以下の様なエラーが出てしまった。\n",
    "\n",
    "![](image/2022-06-28-22-51-13.png)\n",
    "\n",
    "何をやってもこのエラーが解消しない。序盤も序盤であるのに、何が起こっているのか全く分からなかった。皆さんはこの愚かなミスにお気づきだろうか。\n",
    "\n",
    "![](image/2022-06-28-22-53-09.png)\n",
    "\n",
    "なんと、display のはずがコピペミスでdisplaになってしまっていただけだった。それに気づかずdisplaというモジュールについて調べていた時間を返してほしいと思いつつ、単に自分があほなだけであった。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 勾配降下法の実験（p109~）\n",
    "---\n",
    "<br>\n",
    "本書によると、勾配降下法で扱う値は**絶対値が1以下であることが望ましい**という事だった。何故だろうか、というかそんなこと言って、本当は何とかなるのではないかと疑りを掛けて、扱う値から平均値を引かずにコードを実行してみた。\n",
    "\n",
    "![](image/2022-06-29-12-15-16.png)\n",
    "\n",
    "↑X,Yを、そのままの値にして計算してみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### ▪次第に怪しくなる雲行き\n",
    "<br>\n",
    "Yp(Yの予測値)を、学習させる前の段階で出してみた。すると、体重を予想しているはずなのに身長の様な値が出てきてしまった。これは、学習前なので仕方ないかと思い次に進む。\n",
    "\n",
    "![](image/2022-06-29-12-21-08.png)\n",
    "\n",
    "<br>\n",
    "次に、本命の平均二乗誤差である。ここで損失を求めると、かなり大きな値が出てしまった。でも、今後学習をしていけばこの損失も指数関数的に小さくなるだろうと願い次に進む。\n",
    "\n",
    "![](image/2022-06-29-12-24-18.png)\n",
    "\n",
    "<br>\n",
    "勾配計算を行った。WもBも相当大きな値である。とても体重を予想するパラメータではないように思えるが、コンピューターを侮ってはいけない。ここから膨大な学習を経て、最適解に近づくはずである。\n",
    "\n",
    "![](image/2022-06-29-12-27-07.png)\n",
    "\n",
    "<br>\n",
    "学習率を定義し、パラメータを確認してみる。するとどうだろう、少しはまともな値に近づいた気がしなくもない。\n",
    "\n",
    "![](image/2022-06-29-12-30-04.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### ▪衝撃の結末\n",
    "<br>\n",
    "epoch数を500に設定して、学習させてみる。すると…\n",
    "\n",
    "![](image/2022-06-29-12-44-46.png)\n",
    "\n",
    "<br>\n",
    "何と、epoch10にしてすでにlossが発散してしまっている。この結果から得られた結論はこのようになるだろう。\n",
    "\n",
    "- 学習前の損失が大きすぎると学習過程で発散してしまう\n",
    "- データの絶対値を小さくすることで（身長と体重のように）学習データと予測値との桁が異なる場合でも、最初の予測値が論外な値を取らないようになる \n",
    "\n",
    "山登りの例でいうと、一回に進む距離が長すぎるという事だろうか。データの絶対値を小さくする重要性がよく分かった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
