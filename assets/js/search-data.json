{
  
    
        "post0": {
            "title": "『pytorch ＆ 深層学習プログラミング』　入門編",
            "content": "&#30446;&#27425; . np.trsnspose() (p536) | . np.trsnspose() (p536) . . この関数が行っている行列の変換の仕方が分かりづらかった。 . 本書よりもっと単純な行列でその変換の仕方を見る。 . import numpy as np n1 = np.arange(1,13,1) n2 = n1.reshape(2,3,2) n2 . array([[[ 1, 2], [ 3, 4], [ 5, 6]], [[ 7, 8], [ 9, 10], [11, 12]]]) . この行列をnp.transpose関数で変換してみると . n3 = np.transpose(n2, (0,2,1)) print(n3) n3.shape . [[[ 1 3 5] [ 2 4 6]] [[ 7 9 11] [ 8 10 12]]] . (2, 2, 3) . 正直言うと、この変換がどのような法則で行われているのかわからなかった。公式ドキュメントにはこの変換で軸を入れ替えると書いてあるが、それは分かるのである。実際にこの変換がどのような法則によって行われているのか、自分なりにかみ砕いて説明する。 . . &#9642; 3&#27573;&#38542;&#12391;&#22793;&#25563; . 変換は3段階に分けて考えると分かりやすい。 . . 全体の構造を把握 | 一番内側の（最小の）格子 [] の中身が、元の行列のどの部分に相当するか | 次に来る格子が、元の行列のどの部分に相当するか | . である。具体的に上の例ではどういうことなのかは以下の通り . . n3.shapeで構造を確認 | 新しくできた行列の[1,3,5]がどのようにして生成されたかを考える。 | 新しくできた行列の[2,4,6]がどのように生成されたかを考える。 | . &#9642; &#26368;&#21021;&#12395;&#29983;&#25104;&#12373;&#12428;&#12427;&#26684;&#23376;&#12399;&#20309;&#12363; . もう少し具体的に説明する。 . 1．については簡単で、n3.shapeの結果を見ればよいが2．が難しい。2．では「一番最初に生成される格子が何なのか」という事に注目しているが、この際大切なのは行列の最小単位を決定する部分はどこかなので、np.transpose(a,b,c)のcに注目すると良い。今回の場合、np.transpose(0,2,1)なので、1に注目する。この軸１は元の行列で何を示しているのだろうか。 . 今回大事なのは（元の行列において）1,3,5の（縦）方向で行列を作るという意味であると思う。 . . &#9642;1,3,5&#12398;&#65288;&#32294;&#65289;&#26041;&#21521;&#12391;&#34892;&#21015;&#12434;&#20316;&#12427;&#12392;&#12399; . それを考える前に思い出したいのは、今の目標は「なぜ新しく生成された最初の格子が[1,3,5]になったのか」の解明、という事だ。1．で得られた構造も参考にすると、新しくできる行列の形は（2,2,3）であり、何か3つの数を取り出せば最小格子の中身が得られるという事は簡単に読み取れる。 . そしてその「何か3つの数」をどのようにして取り出すか、それが1,3,5の（縦）方向であるという事なのだ。すなわち、新たな行列の最小格子の一つは[1,3,5]であると分かる。 . 縦並べ比較 . . . . &#9642; 2&#12388;&#30446;&#12395;&#29983;&#25104;&#12373;&#12428;&#12427;&#26684;&#23376;&#12399;&#20309;&#12363; . 新たな行列では[2,4,6]が次に来ている。これはどういうことなのかというのが、3．で分かるようになっている。 . 先ほどと同じ要領で、とりあえず[2,4,6]か[7,9,11]を取り出せば良いという事は何となくわかるはずである。では、どちらが正解なのか。 . . &#9642; 2&#12388;&#30446;&#12398;&#26684;&#23376;&#12398;&#21462;&#12426;&#20986;&#12375;&#26041; . 次に来る格子を判断するためにはnp.transpose(a,b,c)のbに注目すると良い。これが次の格子を選ぶ際の軸となる。今回はnp.transpose(0,2,1)なので、2に注目する。軸2について大切なのは（元の行列において）1,2や3,4方向で数えるという意味だけである。このように数え方の方向性が分かれば、元の行列から[1,3,5]の次に取ってくる格子の中身は、1,2方向（横方向）に進んで[2,4,6]にすると理解することが出来るはずである。決して[7,9,11]ではないという事だ。 . 新たにできる行列の形は（2,2,3）なので、以上で（2,3）の1セットは完成である。これをもう1セット作れば、新たな行列の完成だ。 . ちなみに、np.transpose(0,1,2)の0はあまり気にしなくても何とかなるというのが今のところの感想で、大事なのはその後ろの2つの軸であるという事を覚えておきたい。 . . &#9642; &#26412;&#12398;&#20363;&#12391;&#30906;&#35469;&#12375;&#12390;&#12415;&#12427; . 以下が、本書のnp.transposeの変換である。 . n4 = np.arange(0,24,1) n5 = n4.reshape(2,3,4) print(f&#39;n5のshapeは {n5.shape}で、中身は以下の通り&#39;) print(n5) . n5のshapeは (2, 3, 4)で、中身は以下の通り [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] . n6 = np.transpose(n5,(1,2,0)) print(f&#39;n6のshapeは{n6.shape}で、中身は以下の通り&#39;) print(n6) . n6のshapeは(3, 4, 2)で、中身は以下の通り [[[ 0 12] [ 1 13] [ 2 14] [ 3 15]] [[ 4 16] [ 5 17] [ 6 18] [ 7 19]] [[ 8 20] [ 9 21] [10 22] [11 23]]] . また説明するのは大変なので例を載せて終了にするが、分かっただろうか。先ほどと同じように、出来る最小格子は[0,12]や[1,13]や[4,16]などだと分かれば、あとは何とかなるはずである。 .",
            "url": "https:/Shoheihe1.github.io/fanfan/2022/04/08/introweb.html",
            "relUrl": "/2022/04/08/introweb.html",
            "date": " • Apr 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "『pytorch ＆ 深層学習プログラミング』　備忘録chap2",
            "content": "&#30446;&#27425; . 何故スカラーなのか（p83） | 何故スカラーにするためにsum()を用いるのか（p83）を用いるのか（p83）) | . 協力：K.I.先生 . . &#20309;&#25925;&#12473;&#12459;&#12521;&#12540;&#12394;&#12398;&#12363;&#65288;p83&#65289; . pytorchでは、勾配計算をする際に対象の関数がスカラーである必要があるらしい。正直、「スカラーにしてしまったら元の関数がわからなくなるじゃないか！」と思ってしまったのだが、どうやらpytorchはそのスカラーがどのようにして生成されたのかを記憶しているようである。 . pytorchの仕様として基本的にスカラーにしないと勾配計算ができないので、ここは黙ってスカラーにしておくのが無難である。 . &#20309;&#25925;&#12473;&#12459;&#12521;&#12540;&#12395;&#12377;&#12427;&#12383;&#12417;&#12395;sum()&#12434;&#29992;&#12356;&#12427;&#12398;&#12363;&#65288;p83&#65289; . スカラーにしてもその生前の記憶は残っていると分かったものの、次に生じた疑問は「スカラーにするだけならsum()以外の関数でも良いのでは？」というものである。 . 試しにmean()でスカラーにしてからbackward()をかけると、結果は微分の値にはならなかった。つまり、sum()を用いてtensorの合計を求めるという計算には何か重要な意味があるのである。 . . &#9642; &#29702;&#35299;&#12398;&#12383;&#12417;&#12398;&#21069;&#25552;&#30693;&#35672; . それを理解するためには、以下の事を知っておかなければならない。 . xという「ベクトル」に対しても、x.backward() をする時に、（）に引数を入れると計算ができるということ | その引数は、xと同じサイズのベクターにするということ | . である。具体的に本書の例で言うと次のような意味となる。 . z = y.sum() してから z.backward() する代わりに、y.sum() をしないでいきなり y.backward(torch.ones(y.size())) としてもよく、実際に、同じ結果になる。ここで、torch.ones(y.size()) は　yと同じ大きさで、全ての要素が１であるベクトルである。この計算は、「全てのy を同等の weight で扱って x.grad を計算する」という意味になっている。 . . &#9642; z.backward()&#12395;&#38560;&#12373;&#12428;&#12383;&#24341;&#25968;&#65297; . もうお気づきかもしれないが、実は z.backward() のカッコの中には 1 が隠されている。これは z を大きさ 1 のスカラーにしているので考えてみれば当然であるが、それに気づくとsum()の意味が分かってくる。y.sum()を行うことで、均等のweightで全ての要素を足し合わせたことになるのである！ . . &#9642; &#22343;&#31561;&#12398;weight&#12395;&#12424;&#12427;&#36275;&#12375;&#31639; . 最初はこれに突っかかってしまったのだが、それぞれの要素に何も掛け算せず（×0.8 , ×1.3 等）足し合わせているのだから、重みは当然1である。全ての重みを１にすると、微分することと同義となるのである。しかし考えてみると、機械学習やディープラーニングにおいて全ての重みを1にするなど意味のない行為である。今後、この重みを調節することによって高度な学習をするようになるのだろうか、と期待しつつ今後勉強していきたい。 .",
            "url": "https:/Shoheihe1.github.io/fanfan/2022/03/31/chapter2.html",
            "relUrl": "/2022/03/31/chapter2.html",
            "date": " • Mar 31, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "『pytorch ＆ 深層学習プログラミング』　備忘録chap1",
            "content": "この本を読み進める上でつっかかった箇所のみを書いた . &#30446;&#27425; . arange関数 | 関数を微分する関数（p36） | 「r」ってなんだ（p38） | オブジェクト指向とは結局なんなのか | 解析的な微分と数値微分 | . . arange&#38306;&#25968;&#65288;p31&#65289; . 何回も使ってたはずなのに、arange関数の使い方を忘れた。調べると、第一引数（下限値）が閉区間、第二引数（上限値）は開区間になるらしい。 . . np.linspace() . ：この関数では要素数を指定して等差の配列を得ることが出来る。 . 等差数列を得たい時に、交差と要素数のどちらを指定したいかによって関数を使い分けると良い。 . . print(np.arange(-10,10,2)) print(np.linspace(-10,10,2)) . [-10 -8 -6 -4 -2 0 2 4 6 8] [-10. 10.] . . &#38306;&#25968;&#12434;&#24494;&#20998;&#12377;&#12427;&#38306;&#25968;&#65288;p36&#65289; . 微分する関数を作るのは容易で、いつも通り def bibun():のように微分の関数を作れば良い。 . 問題は関数の戻り値を関数にしたいという点。 . これが意外と、良く分かっていないところだった。 . A関数の中にB関数を埋め込み、そのA関数の返り値をB関数にすれば良いというのが解決法で、考えてみればそりゃそうかという感じだった。 . def after_bibun(f): #微分する関数を中に作る def bibun(x): h = 1e-6 return (f(x+h) - f(x-h) / (2*h)) return bibun . . &#12300;r&#12301;&#12387;&#12390;&#12394;&#12435;&#12384;&#65288;p38&#65289; . 突如出てきたlabel =の後に出てきた「r」。これって何だっけと思った。 . 調べたら、これはr以降の文字列を 「確実に文字列である」 と認識させるためのもので、別になくても同じ結果になった。あらゆる環境で動作するために一応書いたのかな？ . . print (&#39;this is a sentence.&#39;) print (&#39; this is a sentence with an indent.&#39;) print (r&#39; this is a sentence with an indent&#39;) # 「r」があるとそれ以降の文字列をエスケープシーケンスとして認識しない . this is a sentence. his is a sentence with an indent. this is a sentence with an indent . このように、事故でエスケープシーケンスと認識されてしまう事を防いだりできる。 . エスケープシーケンス . :「 」と英字を組み合わせて、特殊な意味を持たせられる文字の並び（sequence）の事。上の例なら、thisの「t」という役割から免れる（escape）ことが出来る文字の並びがあるよ、ということだと勝手に理解している。 . . &#12458;&#12502;&#12472;&#12455;&#12463;&#12488;&#25351;&#21521;&#12392;&#12399;&#32080;&#23616;&#12394;&#12435;&#12394;&#12398;&#12363; . . 色々な本にオブジェクト指向の大切さが書かれているけれど、結局何なのか。 . まず、何がオブジェクトなんだろうという疑問がずっとあった。オブジェクトは「モノ」という意味で、pythonにおける「モノ」とは何かが重要だ。調べた結果、それはすなわちクラスから作られたインスタンスのことなのだと分かった。 . オブジェクト＝インスタンス . と考えて差し支えないような気がした。そのインスタンスすなわち「モノ」を使って、プログラムしていく。 . . だからオブジェクト指向とは、まずクラスを作って、そこからインスタンス（オブジェクト）を生成する方向性で開発しようねという考え方なのかなと思った。 . 一般化してからコード作れよ、ってことだろうか。今はそのくらいでいいやと思った。 （参考HP） . &#35299;&#26512;&#30340;&#12394;&#24494;&#20998;&#12392;&#25968;&#20516;&#24494;&#20998; . 初めて聞いた言葉だったけど、これまで知っていた微分の概念にただ名前がついただけだった。 . 解析的な微分は公式に則って微分すること（log(x)の微分は1/xになるとか | 数値微分は微分の定義に従ってlim飛ばして気合で計算すること | . しかし、上記の説明は表層を見ただけの説明にすぎず、根本的な違いは以下参照。 . 無限と有限 . ：実は両者には、無限と有限という対照的な違いがある。 . 機械学習において、数値微分ではlimを飛ばすときのhを限りなく小さいが有限の値に設定する。つまりこの計算は有限であり、また離散的となる。一方解析的な微分は無限を含んだ微分であり、これはコンピューターには難しい計算らしい。 .",
            "url": "https:/Shoheihe1.github.io/fanfan/2022/03/30/chapter1.html",
            "relUrl": "/2022/03/30/chapter1.html",
            "date": " • Mar 30, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https:/Shoheihe1.github.io/fanfan/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https:/Shoheihe1.github.io/fanfan/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}