{
  
    
        "post0": {
            "title": "『pytorch ＆ 深層学習プログラミング』　備忘録3章",
            "content": "&#30446;&#27425; . 0以下は0にするReLU関数（p144） | . . 0&#20197;&#19979;&#12399;0&#12395;&#12377;&#12427;ReLU&#38306;&#25968;&#65288;p144&#65289; . 活性化関数についての説明があり、一次関数しか生まれない線形関数の壁をReLU関数を挟むだけで打ち破ることができると、グラフを交えて説明してある。 . なんて画期的なんだろうと思い、その先の章のReLU関数の説明を見てみた。すると、ReLU関数とは0以下は0に、0以上の数はそのまま返すという単純なものであった。 . それだけで次元の壁を超えられるんだなあと感心しつつ、ふとひっかっかる。「今回の入力値は負の値を含む平均0、標準偏差１の乱数なのに、負の値を0なんかにして正しく予想できるのだろうか」と。 . &#9642;&#20986;&#21147;&#23652;&#12434;&#35211;&#12424; . この疑問は、今回使用しているモデルの構造について考えが浅いために生じていることが判明した。なんと、1つの入力から10の中間層を作っていたのである。 . . 正直、１つの入力値から10の中間層を作るというイメージは湧かない。湧かないが、1つのものを細かく分解して見ているのかなと、1人の人間にも色々な臓器が入っているよねと、その程度の理解をしておく。まあ、人間ならまだしもただの数値１つとなるとやはりイメージが湧かないが仕方ない。 .",
            "url": "https:/Shoheihe1.github.io/fanfan/2022/08/08/chapter4.html",
            "relUrl": "/2022/08/08/chapter4.html",
            "date": " • Aug 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "『pytorch ＆ 深層学習プログラミング』　備忘録3章",
            "content": "&#30446;&#27425; . displaの謎 | 勾配降下法の実験（p109~） | . displa&#12398;&#35598; . テキストのコードが載っているgitからコピペをしてコードを動かそうとすると、以下の様なエラーが出てしまった。 . . 何をやってもこのエラーが解消しない。序盤も序盤であるのに、何が起こっているのか全く分からなかった。皆さんはこの愚かなミスにお気づきだろうか。 . . なんと、display のはずがコピペミスでdisplaになってしまっていただけだった。それに気づかずdisplaというモジュールについて調べていた時間を返してほしいと思いつつ、単に自分があほなだけであった。 . . &#21246;&#37197;&#38477;&#19979;&#27861;&#12398;&#23455;&#39443;&#65288;p109~&#65289; . 本書によると、勾配降下法で扱う値は絶対値が1以下であることが望ましいという事だった。何故だろうか、というかそんなこと言って、本当は何とかなるのではないかと疑りを掛けて、扱う値から平均値を引かずにコードを実行してみた。 . . ↑X,Yを、そのままの値にして計算してみる。 . . &#9642;&#27425;&#31532;&#12395;&#24618;&#12375;&#12367;&#12394;&#12427;&#38642;&#34892;&#12365; . Yp(Yの予測値)を、学習させる前の段階で出してみた。すると、体重を予想しているはずなのに身長の様な値が出てきてしまった。これは、学習前なので仕方ないかと思い次に進む。 . . 次に、本命の平均二乗誤差である。ここで損失を求めると、かなり大きな値が出てしまった。でも、今後学習をしていけばこの損失も指数関数的に小さくなるだろうと願い次に進む。 . . 勾配計算を行った。WもBも相当大きな値である。とても体重を予想するパラメータではないように思えるが、コンピューターを侮ってはいけない。ここから膨大な学習を経て、最適解に近づくはずである。 . . 学習率を定義し、パラメータを確認してみる。するとどうだろう、少しはまともな値に近づいた気がしなくもない。 . . . &#9642;&#34909;&#25731;&#12398;&#32080;&#26411; . epoch数を500に設定して、学習させてみる。すると… . . 何と、epoch10にしてすでにlossが発散してしまっている。この結果から得られた結論はこのようになるだろう。 . 学習前の損失が大きすぎると学習過程で発散してしまう | データの絶対値を小さくすることで（身長と体重のように）学習データと予測値との桁が異なる場合でも、最初の予測値が論外な値を取らないようになる | . 山登りの例でいうと、一回に進む距離が長すぎるという事だろうか。データの絶対値を小さくする重要性がよく分かった。 .",
            "url": "https:/Shoheihe1.github.io/fanfan/2022/06/27/chapter3.html",
            "relUrl": "/2022/06/27/chapter3.html",
            "date": " • Jun 27, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "『Pytorch と fastai で始めるディープラーニング』　備忘録１章",
            "content": "&#30446;&#27425; . 画像のアップロード（p17） | pathオブジェクトの利用（p25） | | . . &#30011;&#20687;&#12398;&#12450;&#12483;&#12503;&#12525;&#12540;&#12489;&#65288;p17&#65289; . 教科書通りに画像をアップロードして分類させようとすると、以下の様なエラーが出る。 . . （備忘録として書いておくと、切り取った画像はpaste image という拡張機能により「ctrl + alt + v」で自動でimage フォルダに画像を保存され、ここに張り付けてくれる。） . どういうことかという事で、一度以下のように試行しても同じエラーが出た。 . どうやらアップロードが上手くいっていないらしい . . &#9642; &#26041;&#37341;&#12434;&#22793;&#12360;&#12390;&#12415;&#12427; . ここで引っかかっているのはもったいないしあまり重要でないはずなので、仕方なく自力でアップロードすることにした。 . image フォルダに自分で画像をアップロードし、それをuploader変数のlistの一つにすることにした。 . . ↑成功。なんとAI様によると100%ねこらしい。機械に100%と言われるのは、何だか逆に心配になってくる。 . . path&#12458;&#12502;&#12472;&#12455;&#12463;&#12488;&#12398;&#21033;&#29992;&#65288;p25&#65289; . 本書のコラムに、pathオブジェクトは便利だ、という記載があったので調べてみた。 . 例えば画像を学習させる際は複数のディレクトリに訓練用、学習用、検証用のデータを入れて、モデル（正しくはアーキテクチャ？）に学習させる。 . この、「画像をそれぞれのディレクトリに割り当てる」という作業が実は面倒だった記憶があるのだが、ディレクトリのパスを単なる文字列ではなくpathオブジェクトにするだけで、色々と便利になるらしい。 . 分かりやすい説明があったので、リンクを載せさせていただいた。 . →narito blog「Python、pathlibモジュールを使う」 .",
            "url": "https:/Shoheihe1.github.io/fanfan/2022/06/05/fastai_chapter1.html",
            "relUrl": "/2022/06/05/fastai_chapter1.html",
            "date": " • Jun 5, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "『pytorch ＆ 深層学習プログラミング』　入門編",
            "content": "&#30446;&#27425; . np.trsnspose() (p536) | . np.trsnspose() (p536) . . この関数が行っている行列の変換の仕方が分かりづらかった。 . 本書よりもっと単純な行列でその変換の仕方を見る。 . import numpy as np n1 = np.arange(1,13,1) n2 = n1.reshape(2,3,2) n2 . array([[[ 1, 2], [ 3, 4], [ 5, 6]], [[ 7, 8], [ 9, 10], [11, 12]]]) . この行列をnp.transpose関数で変換してみると . n3 = np.transpose(n2, (0,2,1)) print(n3) n3.shape . [[[ 1 3 5] [ 2 4 6]] [[ 7 9 11] [ 8 10 12]]] . (2, 2, 3) . 正直言うと、この変換がどのような法則で行われているのかわからなかった。公式ドキュメントにはこの変換で軸を入れ替えると書いてあるが、それは分かるのである。実際にこの変換がどのような法則によって行われているのか、自分なりにかみ砕いて説明する。 . . &#9642; 3&#27573;&#38542;&#12391;&#22793;&#25563; . 変換は3段階に分けて考えると分かりやすい。 . . 全体の構造を把握 | 一番内側の（最小の）格子 [] の中身が、元の行列のどの部分に相当するか | 次に来る格子が、元の行列のどの部分に相当するか | . である。具体的に上の例ではどういうことなのかは以下の通り . . n3.shapeで構造を確認 | 新しくできた行列の[1,3,5]がどのようにして生成されたかを考える。 | 新しくできた行列の[2,4,6]がどのように生成されたかを考える。 | . &#9642; &#26368;&#21021;&#12395;&#29983;&#25104;&#12373;&#12428;&#12427;&#26684;&#23376;&#12399;&#20309;&#12363; . もう少し具体的に説明する。 . 1．については簡単で、n3.shapeの結果を見ればよいが2．が難しい。2．では「一番最初に生成される格子が何なのか」という事に注目しているが、この際大切なのは行列の最小単位を決定する部分はどこかなので、np.transpose(a,b,c)のcに注目すると良い。今回の場合、np.transpose(0,2,1)なので、1に注目する。この軸１は元の行列で何を示しているのだろうか。 . 今回大事なのは（元の行列において）1,3,5の（縦）方向で行列を作るという意味であると思う。 . . &#9642;1,3,5&#12398;&#65288;&#32294;&#65289;&#26041;&#21521;&#12391;&#34892;&#21015;&#12434;&#20316;&#12427;&#12392;&#12399; . それを考える前に思い出したいのは、今の目標は「なぜ新しく生成された最初の格子が[1,3,5]になったのか」の解明、という事だ。1．で得られた構造も参考にすると、新しくできる行列の形は（2,2,3）であり、何か3つの数を取り出せば最小格子の中身が得られるという事は簡単に読み取れる。 . そしてその「何か3つの数」をどのようにして取り出すか、それが1,3,5の（縦）方向であるという事なのだ。すなわち、新たな行列の最小格子の一つは[1,3,5]であると分かる。 . 縦並べ比較 . . . . &#9642; 2&#12388;&#30446;&#12395;&#29983;&#25104;&#12373;&#12428;&#12427;&#26684;&#23376;&#12399;&#20309;&#12363; . 新たな行列では[2,4,6]が次に来ている。これはどういうことなのかというのが、3．で分かるようになっている。 . 先ほどと同じ要領で、とりあえず[2,4,6]か[7,9,11]を取り出せば良いという事は何となくわかるはずである。では、どちらが正解なのか。 . . &#9642; 2&#12388;&#30446;&#12398;&#26684;&#23376;&#12398;&#21462;&#12426;&#20986;&#12375;&#26041; . 次に来る格子を判断するためにはnp.transpose(a,b,c)のbに注目すると良い。これが次の格子を選ぶ際の軸となる。今回はnp.transpose(0,2,1)なので、2に注目する。軸2について大切なのは（元の行列において）1,2や3,4方向で数えるという意味だけである。このように数え方の方向性が分かれば、元の行列から[1,3,5]の次に取ってくる格子の中身は、1,2方向（横方向）に進んで[2,4,6]にすると理解することが出来るはずである。決して[7,9,11]ではないという事だ。 . 新たにできる行列の形は（2,2,3）なので、以上で（2,3）の1セットは完成である。これをもう1セット作れば、新たな行列の完成だ。 . ちなみに、np.transpose(0,1,2)の0はあまり気にしなくても何とかなるというのが今のところの感想で、大事なのはその後ろの2つの軸であるという事を覚えておきたい。 . . &#9642; &#26412;&#12398;&#20363;&#12391;&#30906;&#35469;&#12375;&#12390;&#12415;&#12427; . 以下が、本書のnp.transposeの変換である。 . n4 = np.arange(0,24,1) n5 = n4.reshape(2,3,4) print(f&#39;n5のshapeは {n5.shape}で、中身は以下の通り&#39;) print(n5) . n5のshapeは (2, 3, 4)で、中身は以下の通り [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] . n6 = np.transpose(n5,(1,2,0)) print(f&#39;n6のshapeは{n6.shape}で、中身は以下の通り&#39;) print(n6) . n6のshapeは(3, 4, 2)で、中身は以下の通り [[[ 0 12] [ 1 13] [ 2 14] [ 3 15]] [[ 4 16] [ 5 17] [ 6 18] [ 7 19]] [[ 8 20] [ 9 21] [10 22] [11 23]]] . また説明するのは大変なので例を載せて終了にするが、分かっただろうか。先ほどと同じように、出来る最小格子は[0,12]や[1,13]や[4,16]などだと分かれば、あとは何とかなるはずである。 .",
            "url": "https:/Shoheihe1.github.io/fanfan/2022/04/08/introweb.html",
            "relUrl": "/2022/04/08/introweb.html",
            "date": " • Apr 8, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "『pytorch ＆ 深層学習プログラミング』　備忘録2章",
            "content": "&#30446;&#27425; . 何故スカラーなのか（p83） | 何故スカラーにするためにsumを用いるのか（p83） | graphvizエラーの解決（p82） | . 協力：K.I.先生 . . &#20309;&#25925;&#12473;&#12459;&#12521;&#12540;&#12394;&#12398;&#12363;&#65288;p83&#65289; . pytorchでは、勾配計算をする際に対象の関数がスカラーである必要があるらしい。 . 正直、「スカラーにしてしまったら元の関数がわからなくなるじゃないか！」と思ってしまったのだが、どうやらpytorchはそのスカラーがどのようにして生成されたのかを記憶しているようである。 . pytorchの仕様として基本的にスカラーにしないと勾配計算ができないので、ここは黙ってスカラーにしておくのが無難である。 . &#20309;&#25925;&#12473;&#12459;&#12521;&#12540;&#12395;&#12377;&#12427;&#12383;&#12417;&#12395;sum&#12434;&#29992;&#12356;&#12427;&#12398;&#12363;&#65288;p83&#65289; . スカラーにしてもその生前の記憶は残っていると分かったものの、次に生じた疑問は . 「スカラーにするだけならsum()以外の関数でも良いのでは？」 . というものである。 . 試しにmean()でスカラーにしてからbackward()をかけると、結果は微分の値にはならなかった。つまり、sum()を用いてtensorの合計を求めるという計算には何か重要な意味があるはずだと分かる。 . 図．mean関数を使ったときのグラフ . . 図．sum関数を使ったときのグラフ . . 両者を見比べると、何かが分かるような気がする。が、分からない。 . . &#9642; &#29702;&#35299;&#12398;&#12383;&#12417;&#12398;&#21069;&#25552;&#30693;&#35672; . それを理解するためには、以下の事を知っておかなければならない。 . xという「ベクトル」に対しても、x.backward() をする時に、（）に引数を入れると計算ができるということ | その引数は、xと同じサイズのベクターにするということ | . である。例えばこのテキストでは、 . z = y.sum() 　→ 　z.backward() . としているが、その代わりにいきなり y.backward(torch.ones(y.size())) としてもよく、実際同じ結果になる。（ y.sum() はしない） . ここで torch.ones(y.size()) は　y と同じ大きさで、全ての要素が１であるベクトルである。この計算は「全ての y を同等の weight で扱って x.grad を計算する」という意味になっている。 . . &#9642; z.backward()&#12395;&#38560;&#12373;&#12428;&#12383;&#24341;&#25968;&#65297; . もうお気づきかもしれないが、実は z.backward() のカッコの中には 1 が隠されている。これは z を大きさ 1 のスカラーにしているので考えてみれば当然であるが、それに気づくとsum()の意味が分かってくる。 . y.sum()を行うことでどんなtensorであっても確実に大きさ１のスカラーに変換し、同時に均等のweightで全ての要素を足し合わせたことになるのである！ . . &#9642; &#22343;&#31561;&#12398;weight&#12395;&#12424;&#12427;&#36275;&#12375;&#31639; . 最初はこれに突っかかってしまったのだが、それぞれの要素に何も掛け算せず（×0.8 , ×1.3 等）足し合わせているのだから、重みは当然1である。全ての重みを１にすると、微分することと同義となるのだ。 . しかし考えてみると、機械学習やディープラーニングにおいて全ての重みを1にするなど意味のない行為である。今後、この重みを調節することによって高度な学習をするようになるのだろうか、と期待しつつ今後勉強していきたい。 . . graphviz&#12456;&#12521;&#12540;&#12398;&#35299;&#27770;&#65288;p82&#65289; . なんと、久々に82ページのコードを実行しようとすると、下記の様なエラーが出てしまった。このエラーを解決するためには、 . 1．仮想環境の作成　→　2．graphvizのインストール　→　3.環境設定でPATHの新規作成 . という3段階を経なければならない。なんだかんだ4時間以上かかってしまった。 . . エラー内容 ： …failed to execute WindowsPath(&#39;dot&#39;), make sure the Graphviz executables are on your systems&#39; PATH . 1.&#20206;&#24819;&#29872;&#22659;&#12398;&#20316;&#25104; . VScodeにおいて仮想環境を作る場合、ターミナルをVScode内で開くことができるので少し楽だ。手順は以下に示す。 . 仮想環境を作成する適当なフォルダを作成する。(例． Users username imaginary_environment graphviz ディレクトリを作る) | VScodeにて｢F1｣を押し、｢Create Terminal｣を選択しターミナルを開く | ｢py -3 -m venv .venv｣を作成したディレクトリで実行する（windowsの場合） | ターミナルを一度閉じ、再度開く。自分の場合ExecutionPolicyのエラーが出てしまったため、｢Set-ExecutionPolicy RemoteSigned -Scope Process｣をターミナルで毎度入力し、権限を一時的に許可している | . これで仮想環境の作成は完了である。あとは必要な物をインストールすれば良い . . 2.graphviz&#12398;&#12452;&#12531;&#12473;&#12488;&#12540;&#12523; . ここで落とし穴があり、py -m pip install torchvizをターミナルで行ったとしてもインストールしてあるgraphvizとの繋がりが形成されるだけで、graphviz自体はインストールされないという事だ。 . つまり、graphvizは公式サイトから自分でインストールしなければならない。 . . 3.&#29872;&#22659;&#35373;&#23450;&#12391;PATH&#12398;&#26032;&#35215;&#20316;&#25104; . インストールしただけでは、実はまだエラーは解消しない。エラー文をよく読むと、PATHが何たらと書いてある。これが最後の問題点である。 . 自分も良く分かっていないが、PATHとは探し物をする際の道しるべのようなもので、ここに登録しておいた道（PATH）はコンピューターが毎度歩いて探し物をしてくれるらしい。 . コントロールパネルから「システム」を選択しシステムの詳細設定、そして「環境変数」を選択する。 . そこでユーザーの環境変数において新規を選択し、変数名にはPATH、変数値にはGraphviz binが収納してあるディレクトリを入力すれば、ようやくエラーの解消である。 .",
            "url": "https:/Shoheihe1.github.io/fanfan/2022/03/31/chapter2.html",
            "relUrl": "/2022/03/31/chapter2.html",
            "date": " • Mar 31, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "『pytorch ＆ 深層学習プログラミング』　備忘録1章",
            "content": "この本を読み進める上でつっかかった箇所のみを書いた . &#30446;&#27425; . arange関数 | 関数を微分する関数（p36） | 「r」ってなんだ（p38） | オブジェクト指向とは結局なんなのか | 解析的な微分と数値微分 | . . arange&#38306;&#25968;&#65288;p31&#65289; . 何回も使ってたはずなのに、arange関数の使い方を忘れた。調べると、第一引数（下限値）が閉区間、第二引数（上限値）は開区間になるらしい。 . . np.linspace() . ：この関数では要素数を指定して等差の配列を得ることが出来る。 . 等差数列を得たい時に、交差と要素数のどちらを指定したいかによって関数を使い分けると良い。 . . print(np.arange(-10,10,2)) print(np.linspace(-10,10,2)) . [-10 -8 -6 -4 -2 0 2 4 6 8] [-10. 10.] . . &#38306;&#25968;&#12434;&#24494;&#20998;&#12377;&#12427;&#38306;&#25968;&#65288;p36&#65289; . 微分する関数を作るのは容易で、いつも通り def bibun():のように微分の関数を作れば良い。 . 問題は関数の戻り値を関数にしたいという点。 . これが意外と、良く分かっていないところだった。 . A関数の中にB関数を埋め込み、そのA関数の返り値をB関数にすれば良いというのが解決法で、考えてみればそりゃそうかという感じだった。 . def after_bibun(f): #微分する関数を中に作る def bibun(x): h = 1e-6 return (f(x+h) - f(x-h) / (2*h)) return bibun . . &#12300;r&#12301;&#12387;&#12390;&#12394;&#12435;&#12384;&#65288;p38&#65289; . 突如出てきたlabel =の後に出てきた「r」。見たことはある気がしたが意味が分からなかった。 . 調べると、これはr以降の文字列を 「確実に文字列である」 と認識させるためのもので、別になくても同じ結果になった。あらゆる環境で動作するために一応書いたのかな？ . . print (&#39;this is a sentence.&#39;) print (&#39; this is a sentence with an indent.&#39;) print (r&#39; this is a sentence with an indent&#39;) # 「r」があるとそれ以降の文字列をエスケープシーケンスとして認識しない . this is a sentence. his is a sentence with an indent. this is a sentence with an indent . このように、事故でエスケープシーケンスと認識されてしまう事を防いだりできる。 . エスケープシーケンス . :「 」と英字を組み合わせて、特殊な意味を持たせられる文字の並び（sequence）の事。上の例なら、thisの「t」という役割から免れる（escape）ことが出来る文字の並びがあるよ、ということだと勝手に理解している。 . . &#12458;&#12502;&#12472;&#12455;&#12463;&#12488;&#25351;&#21521;&#12392;&#12399;&#32080;&#23616;&#12394;&#12435;&#12394;&#12398;&#12363; . . 色々な本にオブジェクト指向の大切さが書かれているけれど、結局何なのか。 . まず、何がオブジェクトなんだろうという疑問がずっとあった。オブジェクトは「モノ」という意味で、pythonにおける「モノ」とは何かが重要だ。調べた結果、それはすなわちクラスから作られたインスタンスのことなのだと分かった。 . オブジェクト＝インスタンス . と考えて差し支えないような気がした。そのインスタンスすなわち「モノ」を使って、プログラムしていく。 . . だからオブジェクト指向とは、まずクラスを作って、そこからインスタンス（オブジェクト）を生成する方向性で開発しようねという考え方なのかなと思った。 . 一般化してからコード作れよ、ということだろうか。今はそのくらいでいいやと思った。 （参考HP） . &#35299;&#26512;&#30340;&#12394;&#24494;&#20998;&#12392;&#25968;&#20516;&#24494;&#20998; . 初めて聞いた言葉だったが、これまで知っていた微分の概念にただ名前がついただけだった。 . 解析的な微分は公式に則って微分すること（log(x)の微分は1/xになるとか | 数値微分は微分の定義に従ってlim飛ばして気合で計算すること | . しかし、上記の説明は表層を見ただけの説明にすぎず、根本的な違いは以下参照。 . 無限と有限 . ：実は両者には、無限と有限という対照的な違いがある。 . 機械学習において、数値微分ではlimを飛ばすときのhを限りなく小さいが有限の値に設定する。つまりこの計算は有限であり、また離散的となる。一方解析的な微分は無限を含んだ微分であり、これはコンピューターには難しい計算らしい。 .",
            "url": "https:/Shoheihe1.github.io/fanfan/2022/03/30/chapter1.html",
            "relUrl": "/2022/03/30/chapter1.html",
            "date": " • Mar 30, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "いつの日か画像認識の技術で胆管がんの早期発見が出来たらいいなと思っています。 . 今は特に書くこともないので、寿司打の最高得点が出たら更新していくことにします。 . 2022年6月7日 .",
          "url": "https:/Shoheihe1.github.io/fanfan/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https:/Shoheihe1.github.io/fanfan/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}